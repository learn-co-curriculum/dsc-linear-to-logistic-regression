{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# A real-world example"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Now you will apply these concepts to a real-world dataset: "]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["import statsmodels as sm\n","import sklearn.preprocessing as preprocessing\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split\n","from scipy import stats"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Age</th>\n","      <th>Education</th>\n","      <th>Occupation</th>\n","      <th>Relationship</th>\n","      <th>Race</th>\n","      <th>Sex</th>\n","      <th>Target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>39</td>\n","      <td>Bachelors</td>\n","      <td>Adm-clerical</td>\n","      <td>Not-in-family</td>\n","      <td>White</td>\n","      <td>Male</td>\n","      <td>&lt;=50K</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>50</td>\n","      <td>Bachelors</td>\n","      <td>Exec-managerial</td>\n","      <td>Husband</td>\n","      <td>White</td>\n","      <td>Male</td>\n","      <td>&lt;=50K</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>38</td>\n","      <td>HS-grad</td>\n","      <td>Handlers-cleaners</td>\n","      <td>Not-in-family</td>\n","      <td>White</td>\n","      <td>Male</td>\n","      <td>&lt;=50K</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>53</td>\n","      <td>11th</td>\n","      <td>Handlers-cleaners</td>\n","      <td>Husband</td>\n","      <td>Black</td>\n","      <td>Male</td>\n","      <td>&lt;=50K</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>28</td>\n","      <td>Bachelors</td>\n","      <td>Prof-specialty</td>\n","      <td>Wife</td>\n","      <td>Black</td>\n","      <td>Female</td>\n","      <td>&lt;=50K</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Age  Education         Occupation   Relationship   Race     Sex Target\n","0   39  Bachelors       Adm-clerical  Not-in-family  White    Male  <=50K\n","1   50  Bachelors    Exec-managerial        Husband  White    Male  <=50K\n","2   38    HS-grad  Handlers-cleaners  Not-in-family  White    Male  <=50K\n","3   53       11th  Handlers-cleaners        Husband  Black    Male  <=50K\n","4   28  Bachelors     Prof-specialty           Wife  Black  Female  <=50K"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["salaries = pd.read_csv('salaries_final.csv', index_col=0)\n","salaries.head()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["For this example, you will fit a logistic regression model to `Target` using `Age`, `Race`, and `Sex`. Since `Target`, `Race`, and `Sex` are categorical, they need to be be converted to a numeric datatype first. \n","\n","The `get_dummies()` function will only convert `object` and `category` datatypes to dummy variables so it is safe to pass `Age` to `get_dummies()`. Note that we also pass two additional arguments, `drop_first=True` and `dtype=float`. The `drop_first=True` argument removes the first level for each categorical variable and the `dtype=float` argument converts the datatype of all the dummy variables to float. The data must be float in order to obtain accurate statistical results from `statsmodels`. "]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["# Convert race and sex using get_dummies() \n","x_feats = ['Race', 'Sex', 'Age']\n","X = pd.get_dummies(salaries[x_feats], drop_first=True, dtype=float)\n","\n","# Convert target using get_dummies\n","y = pd.get_dummies(salaries['Target'], drop_first=True, dtype=float)\n","y = y['>50K']"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Optimization terminated successfully.\n","         Current function value: 0.498651\n","         Iterations 6\n"]},{"name":"stderr","output_type":"stream","text":["//anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n","  return ptp(axis=axis, out=out, **kwargs)\n"]}],"source":["import statsmodels.api as sm\n","\n","# Create intercept term required for sm.Logit, see documentation for more information\n","X = sm.add_constant(X)\n","\n","# Fit model\n","logit_model = sm.Logit(y, X)\n","\n","# Get results of the fit\n","result = logit_model.fit()"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"data":{"text/html":["<table class=\"simpletable\">\n","<caption>Logit Regression Results</caption>\n","<tr>\n","  <th>Dep. Variable:</th>         <td>>50K</td>       <th>  No. Observations:  </th>  <td> 32561</td> \n","</tr>\n","<tr>\n","  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td> 32554</td> \n","</tr>\n","<tr>\n","  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>     6</td> \n","</tr>\n","<tr>\n","  <th>Date:</th>            <td>Wed, 20 Nov 2019</td> <th>  Pseudo R-squ.:     </th>  <td>0.09666</td>\n","</tr>\n","<tr>\n","  <th>Time:</th>                <td>14:55:31</td>     <th>  Log-Likelihood:    </th> <td> -16237.</td>\n","</tr>\n","<tr>\n","  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -17974.</td>\n","</tr>\n","<tr>\n","  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>  <td> 0.000</td> \n","</tr>\n","</table>\n","<table class=\"simpletable\">\n","<tr>\n","             <td></td>                <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n","</tr>\n","<tr>\n","  <th>const</th>                   <td>   -4.4248</td> <td>    0.189</td> <td>  -23.380</td> <td> 0.000</td> <td>   -4.796</td> <td>   -4.054</td>\n","</tr>\n","<tr>\n","  <th>Age</th>                     <td>    0.0387</td> <td>    0.001</td> <td>   38.530</td> <td> 0.000</td> <td>    0.037</td> <td>    0.041</td>\n","</tr>\n","<tr>\n","  <th>Race_Asian-Pac-Islander</th> <td>    0.9991</td> <td>    0.197</td> <td>    5.079</td> <td> 0.000</td> <td>    0.614</td> <td>    1.385</td>\n","</tr>\n","<tr>\n","  <th>Race_Black</th>              <td>    0.1812</td> <td>    0.191</td> <td>    0.950</td> <td> 0.342</td> <td>   -0.193</td> <td>    0.555</td>\n","</tr>\n","<tr>\n","  <th>Race_Other</th>              <td>   -0.1143</td> <td>    0.282</td> <td>   -0.406</td> <td> 0.685</td> <td>   -0.667</td> <td>    0.438</td>\n","</tr>\n","<tr>\n","  <th>Race_White</th>              <td>    0.8742</td> <td>    0.183</td> <td>    4.782</td> <td> 0.000</td> <td>    0.516</td> <td>    1.232</td>\n","</tr>\n","<tr>\n","  <th>Sex_Male</th>                <td>    1.2069</td> <td>    0.035</td> <td>   34.380</td> <td> 0.000</td> <td>    1.138</td> <td>    1.276</td>\n","</tr>\n","</table>"],"text/plain":["<class 'statsmodels.iolib.summary.Summary'>\n","\"\"\"\n","                           Logit Regression Results                           \n","==============================================================================\n","Dep. Variable:                   >50K   No. Observations:                32561\n","Model:                          Logit   Df Residuals:                    32554\n","Method:                           MLE   Df Model:                            6\n","Date:                Wed, 20 Nov 2019   Pseudo R-squ.:                 0.09666\n","Time:                        14:55:31   Log-Likelihood:                -16237.\n","converged:                       True   LL-Null:                       -17974.\n","Covariance Type:            nonrobust   LLR p-value:                     0.000\n","===========================================================================================\n","                              coef    std err          z      P>|z|      [0.025      0.975]\n","-------------------------------------------------------------------------------------------\n","const                      -4.4248      0.189    -23.380      0.000      -4.796      -4.054\n","Age                         0.0387      0.001     38.530      0.000       0.037       0.041\n","Race_Asian-Pac-Islander     0.9991      0.197      5.079      0.000       0.614       1.385\n","Race_Black                  0.1812      0.191      0.950      0.342      -0.193       0.555\n","Race_Other                 -0.1143      0.282     -0.406      0.685      -0.667       0.438\n","Race_White                  0.8742      0.183      4.782      0.000       0.516       1.232\n","Sex_Male                    1.2069      0.035     34.380      0.000       1.138       1.276\n","===========================================================================================\n","\"\"\""]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["result.summary()"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"data":{"text/plain":["const                      0.011977\n","Age                        1.039480\n","Race_Asian-Pac-Islander    2.715861\n","Race_Black                 1.198638\n","Race_Other                 0.891987\n","Race_White                 2.396965\n","Sex_Male                   3.343142\n","dtype: float64"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["np.exp(result.params)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["You can also use scikit-learn to retrieve the parameter estimates. The disadvantage here though is that there are no p-values for your parameter estimates!"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"text/plain":["LogisticRegression(C=1000000000000000.0, class_weight=None, dual=False,\n","                   fit_intercept=False, intercept_scaling=1, l1_ratio=None,\n","                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',\n","                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n","                   warm_start=False)"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["logreg = LogisticRegression(fit_intercept = False, C = 1e15, solver='liblinear')\n","model_log = logreg.fit(X, y)\n","model_log"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"data":{"text/plain":["array([[-4.38706342,  0.03871011,  0.96178902,  0.14397983, -0.14384057,\n","         0.83689457,  1.2067121 ]])"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["model_log.coef_"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Summary \n","\n","In this lab you built upon your previous knowledge of linear regression and built an intuitive understanding of how this could be adapted for classification. We then demonstrated tools for performing logistic regression. In the upcoming lessons you will continue to investigate logistic regression from other viewpoints."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":2}
